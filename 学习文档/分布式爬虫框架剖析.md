# 分布式爬虫框架剖析

## 案例代码

```python
# -*- coding:utf-8 -*-
# import basetest
import itertools
import json
import os
import random
import re
import time
import traceback
import uuid
import zlib
import datetime
import pika
import requests
from pika.channel import Channel
import logging
import warnings

num = 0
warnings.filterwarnings("ignore")
# token = "vG3ASQB2o6VT9CrDWhbB75ZIf2xww8f8i_equal_uhQLHYu_plus__plus__equal_pyIto7pvszwPuF9bVS3cWOb5zU54TfU="
logging.basicConfig(level=logging.INFO,  # 对root logger进行一次性配置，日志级别为INFO
                    format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',
                    datefmt='%m-%d %H:%M:%S')  # 设置日期格式 月日 时分秒


# 定义了save函数,用于将抓取的数据提交到后台保存
def save(task, datas):
    global num
    dic = dict()
    dic['sourceId'] = task['SourceId']
    dic['configId'] = task['ConfigId']
    dic['parentId'] = task['ParentID']
    dic['taskId'] = task['TaskID']
    dic['forceCommit'] = 'true'
    dic['datas'] = datas
    payload = json.dumps(dic, ensure_ascii=False)
    print('SAVE CNT', str(len(datas)))
    headers = {
        'Content-Type': 'application/json; charset=utf-8'
    }
    # 采集服务的后台地址
    url = "http://121.11.105.164:9102/api2/v1/saver/save"
    response = requests.request("POST", url, headers=headers, data=payload.encode('utf8'))
    ret = response.text
    num += 1
    print("采集数据：" + str(num))
    print(ret)
    return ret


# 定义了crawler函数,包含具体的网页抓取逻辑,从目标网页抓取内容,提取字段,组装成dict数据,并调用save提交保存
def crawler(taskBean):
    url = 'http://www.xinhuanet.com/2021-08/21/c_1127783368.htm'
    pagehtml = requests.get(url).text
    text = pagehtml.split('<div class="main-aticle" id="detail">')[-1].split('</div>')[0]
    topic_list = text.split('<p> </p>')
    for t in range(len(topic_list)):
        new_string = [
            s.replace('\r', '').replace('<p>', '').replace('<strong>', '').replace('<\p>', '').replace('</strong>','').replace('\u3000','').replace('<font color="#993300">', '').replace('</font>', '').replace('</p>', '').replace('<font color="navy">','')
            for s in topic_list[t].split('\n')
        ]
        print(new_string)
        if t == 0:
            POSTdata = {
                '1102443': new_string[2],
                '1102444': datetime.datetime.strptime(new_string[3][3:], "%Y年%m月%d日").strftime("%Y-%m-%d"),
                '1102445': new_string[4][3:],
                '1102446': "".join(new_string[6:]),
                '1102447': f'{datetime.date.today()}'
            }
        else:
            POSTdata = {
                '1102443': new_string[1],
                '1102444': datetime.datetime.strptime(new_string[2][3:], "%Y年%m月%d日").strftime("%Y-%m-%d"),
                '1102445': new_string[3][3:],
                '1102446': "".join(new_string[5:]),
                '1102447': f'{datetime.date.today()}'
            }
        print(POSTdata)
        save(taskBean, POSTdata)

# 定义了MqHuaZhu类,用于通过MQ接收任务和返回结果
class MqHuaZhu(object):
    # 接收任务的mq主题
    QUEUE_THIS = 'dev_web_snatch_CFWC'
	
    # callback方法中包含抓取任务的主要逻辑,调用crawler函数进行抓取,并将结果 publish 到返回队列
    def callback(self, ch: Channel, method, properties, body):
        try:
            logging.info(" [x] Received %r" % len(body))
            task = json.loads(body)
            global taskBean
            taskBean = task['Task']
            url = taskBean['Url']
            print('do url', url)
            print(taskBean)
            crawler(taskBean)
            print('=========')
            # 将结果string 写入  biz_msg_ret 队列
            #
            # {"IsSuccess": false,"StrTaskId": "57941680","ErrorMessage" : ""}
            message = dict()
            message['IsSuccess'] = 'true'
            message['StrTaskId'] = taskBean['TaskID']
            message['ErrorMessage'] = '-'
            retMsg = json.dumps(message, ensure_ascii=False)
            logging.warning('ret msg, %s ', retMsg)
            ch.basic_publish(exchange='', routing_key='biz_msg_ret', body=retMsg)
            ch.basic_ack(delivery_tag=method.delivery_tag)
            time.sleep(0.5)
        except Exception as e:
            logging.error(e)
            ch.basic_reject(delivery_tag=method.delivery_tag)

    # listener方法用于启动MQ监听,接收任务队列的消息,调用callback进行处理
    def listener(self):
        self.auth = pika.PlainCredentials('admin', 'admin@123')
        self.connection = pika.BlockingConnection(pika.ConnectionParameters(host='121.11.105.164',
                                                                            port=9672,
                                                                            credentials=self.auth,
                                                                            heartbeat=0,
                                                                            ))
        self.channel = self.connection.channel()
        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(on_message_callback=self.callback,
                                   queue=self.QUEUE_THIS,
                                   auto_ack=False)
        logging.info(' [*] Waiting for messages. To exit press CTRL+C')
        self.channel.start_consuming()


if __name__ == '__main__':
    # 主程序创建MqHuaZhu实例并调用listener方法启动监听。
    MqHuaZhu().listener()

```

## 代码分析

这是一个典型的通过MQ分发任务的分布式爬虫方案。

主要优点是:

- 可以动态扩展爬虫实例,提高吞吐量

- MQ保证任务分发的可靠性

- crawler函数聚焦爬虫逻辑,程序结构清晰

如果需要进一步提高爬取效率,可以考虑:

- 使用多线程或协程并发爬取

- 优化任务分配策略(如优先级、时间等)

- 使用持久化队列避免消息丢失

- 加入更多异常处理

总体来说,这是一个设计合理的分布式爬虫程序。

## 框架剖析

该分布式爬虫的框架设计:

1. 整体框架

- 使用MQ实现任务分发和结果收集,Crawler节点作为消费端,管理节点作为生产端

- Crawler节点基于callback作为消费回调,进行具体网页爬取工作

- save函数用于数据提交,crawler函数实现爬取逻辑

- 框架灵活可扩展,可以方便增加Crawler节点

2. MQ机制

- 使用了RabbitMQ作为中间件,提供可靠的异步任务分发

- 生产者发布任务到dev_web_snatch_CFWC队列

- 消费者订阅该队列,自动调度进行处理

- callback处理完成后将结果发布到biz_msg_ret队列

- 基于异步处理,提高吞吐量

3. 爬取流程

- crawler包含具体页面解析逻辑,获取字段信息

- 将解析结果组装为dict,提交到后台

- 处理了文本清洗,提取日期时间等操作

- 可以考虑加入缓存、请求控制、代理等机制

4. 代码结构

- 类的划分合理清晰,函数职责明确

- 好的注释和日志帮助理解

- 可以进一步优化异常处理

- 增加类型提示可以更好地声明接口

总之,这个框架设计合理,实现了爬虫逻辑与分布式任务的解耦,可以作为分布式爬虫的参考模板。后续可以从稳定性、效率等方面进行优化改进。